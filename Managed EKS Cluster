Create an EKS cluster and deploy game into that cluster
==================================================

Task 1: Create an EKS cluster
=============================
Name: <yourname>-eks-cluster-1
Use K8S version to the latest

Create an IAM role 'eks-cluster-role' with 1 policy attached: AmazonEKSClusterPolicy
Create another IAM role 'eks-node-grp-role' with 3 policies attached: 
(Allows EC2 instances to call AWS services on your behalf.)
    - AmazonEKSWorkerNodePolicy
    - AmazonEC2ContainerRegistryReadOnly
    - AmazonEKS_CNI_Policy

Choose default VPC, Choose 3 subnets(atleast one of a different availability zone), once created enable public ipv4 by navigating to the Actions menu select Modify auto-assign IP settings.
Choose a security group which open the ports 22-SSH, 80-HTTP, 8080-CUSTOM TCP
cluster endpoint access: public

# For VPC CNI, CoreDNS and kube-proxy, choose the default versions, For CNI, latest and default are 
# different. But go with default.

Click 'Create'. This process will take 10-12 minutes. Wait till your cluster shows up as Active. 


Task 2: Add Node Groups to our cluster
======================================
Now, lets add the worker nodes where the pods can run

Open the cluster > Compute > Add NodeGrp
Name: <yourname>-eks-nodegrp-1 
Select the role you already created
Leave default values for everything else

AMI - choose the default 1 (Amazon Linux 2)
change desired/minimum/maximum to 1 (from 2)
Enable SSH access. Choose a security group which allows 22, 80, 8080

Choose default values for other fields 

Node group creation may take 2-3 minutes


Task 3: Authenticate to this cluster
===================================
Reference:
https://docs.aws.amazon.com/eks/latest/userguide/create-kubeconfig.html

Open cloudshell

# Type on your AWS CLI window 
aws sts get-caller-identity
# observe your account and user id details

[cloudshell-user@ip-10-130-1-81 ~]$ aws eks list-clusters
{
    "clusters": [
        "cluster-1"
    ]
}

# Create a  kubeconfig file where it stores the credentials for EKS:
# kubeconfig configuration allows you to connect to your cluster using the kubectl command line.
aws eks update-kubeconfig --region region-code --name my-cluster
ex: aws eks update-kubeconfig --region us-east-1 --name unus-eks-cluster-1 # Use the cluster name you just 
created

//Region name should be same as you aws account.

# see if you can get the nodes you created
kubectl get nodes

# Install nano editor in cloudshell. We will need this in the next task
sudo yum install nano -y



Task 4: Create a new POD in EKS for the 2048 game
================================================

# clean up the files in cloudshell (Optional)
rm *.* 

# create the config file in YAML to deploy 2048 game pod into the cluster
nano 2048-pod.yaml

### code starts ###
apiVersion: v1
kind: Pod
metadata:
   name: 2048-pod
   labels:
      app: 2048-ws
spec:
   containers:
   - name: 2048-container
     image: blackicebird/2048
     ports:
       - containerPort: 80

### code ends ###
To save the file Ctrl+O followed by Enter followed by Ctrl+X.


# apply the config file to create the pod
kubectl apply -f 2048-pod.yaml
#pod/2048-pod created

# view the newly created pod
kubectl get pods


Task 5: Setup Load Balancer Service
===================================
nano mygame-svc.yaml  

### code starts ###

apiVersion: v1
kind: Service
metadata:
   name: mygame-svc
spec:
   selector:
      app: 2048-ws
   ports:
   - protocol: TCP
     port: 80
     targetPort: 80
   type: LoadBalancer

### code ends ###

# apply the config file
kubectl apply -f mygame-svc.yaml

# view details of the modified service
kubectl describe svc mygame-svc

# Access the LoadBalancer Ingress on the kops instance
curl <LoadBalancer_Ingress>:<Port_number>
or
curl a06aa56b81f5741268daca84dca6b4f8-694631959.us-east-1.elb.amazonaws.com:80
(try this from your laptop, not from your cloudshell)

# Go to EC2 console. get the DNS name of ELB and paste the DNS into address bar of the browser
# It will show the 2048 game. You can play. (need to wait for 2-3 minutes for the 
# setup to be complete)


Task 3: Cleanup
---------------
# Clean up all the resources created in the task
kubectl get pods
kubectl delete -f 2048-pod.yaml

kubectl get services
kubectl delete -f mygame-svc.yaml
####################################################################
// Steps followed by me are below.

####################################################################
[cloudshell-user@ip-10-130-1-81 ~]$ aws sts get-caller-identity
{
    "UserId": "203230122002",
    "Account": "203230122002",
    "Arn": "arn:aws:iam::203230122002:root"
}                                                                                                      

[cloudshell-user@ip-10-130-1-81 ~]$ aws eks list-clusters
{
    "clusters": [
        "cluster-1"
    ]
}

[cloudshell-user@ip-10-130-1-81 ~]$ aws eks update-kubeconfig --region ap-south-1 --name cluster-1
Added new context arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1 to /home/cloudshell-user/.kube/config

[cloudshell-user@ip-10-130-1-81 ~]$ cat .kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJS01JRC9Kdk9YdFF3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TkRBNU1URXlNVFEwTXpCYUZ3MHpOREE1TURreU1UUTVNekJhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUUMvR2t3V0ZFejNvRTBRUDJxZTIyRm5sUTE2bExlLzVOa3NJTVFvVHcxN2FPWmVCTGw4U1pBdzd5REoKeEMvVkN5UlY3bWw0V0xyeW1QVlJydjVjUUViQ0xtTE4xR0lmc3NXVzJNR05wNDlHOTVKc2w2MmlRQ1JuWXZOWQpIdkw3cHNNakNUODFDVmZmaC9UUzVrSXBZMThnMlFVZXBDS2dPZnV5OEtQN3prcnJiRnh0ZXFxUGNvbmNiYWxkClNRcE45MXRWdVZUekViNkY1QjJNQjdVeDVOR3NHMDAwMkZpbkEzR2IvSGhwT0xpTnA2ZTNEWDRFOTAvS25ITkgKNGVTR2R0R3NLdTVOTjlhUERjOHFBdUE2TXFVWnlLT3R3WG5oeFJmbGI5Y1pyRE9wWERYWmxjWlFHL2UxekY2MAozbmRTcHR2K1dYR1J3dk0zZzk5RkxabTcvSTJoQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJRbFZsOUZMMU9vLzRTK0JMaUtNdDZBL0xqM1lqQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQXkxVVB0blNqRwpJUEpGZC84cWFzWll0TTRMcFFoUmxkT1hQN0tSditTZnNoLzI2M0F5THNYaWkyM3dlSXQrWGt3cHFXYm5MNmhzCldRZDE1QlZRckpSNVMrc2xOSWdzZEJnaHEyUmgwVlUvRkNBcmsxVnpFRFBTK0t6ajFKeGVwZDI5bFZyaXJaS1gKV1o5WUw4MGNxeTJ0NlZhRjNGQ2NOcGNZVzg2MlY0N2QwTzJ1NVJmS0FDbFg3TXp4Si9nOVJQclVpUkFvOG44bgppN1dTbzIrNUxDY3g0SXdwbVA1R29PN2dySXd1bC9lRHhZTGgzeDJjVkl0cWpVS3UzZjNIRGEvaWRFaktISXdaCjF2N2dyVEhZdVhKS1E4V2FTbzVyT2NodjNVV3hXbURlRlgyUWJ2TkFqQnBXRUdNZ3loOVB2akJ5Sk9KTERyOWcKM3pjVGw0N0ozek9UCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://76E7FB2B844A17FACBB4801F4DE71546.yl4.ap-south-1.eks.amazonaws.com
  name: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
contexts:
- context:
    cluster: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
    user: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
  name: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
current-context: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
kind: Config
preferences: {}
users:
- name: arn:aws:eks:ap-south-1:203230122002:cluster/cluster-1
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1beta1
      args:
      - --region
      - ap-south-1
      - eks
      - get-token
      - --cluster-name
      - cluster-1
      - --output
      - json
      command: aws
      
[cloudshell-user@ip-10-130-1-81 ~]$ sudo yum install nano -y                                                                                                                                           
Last metadata expiration check: 0:18:45 ago on Wed 11 Sep 2024 09:56:06 PM UTC.
Package nano-5.8-3.amzn2023.0.4.x86_64 is already installed.
Dependencies resolved.
Nothing to do.
Complete!

[cloudshell-user@ip-10-130-1-81 ~]$ nano 2048-pod.yaml

[cloudshell-user@ip-10-130-1-81 ~]$ cat 2048-pod.yaml

apiVersion: v1
kind: Pod
metadata:
   name: 2048-pod
   labels:
      app: 2048-ws
spec:
   containers:
   - name: 2048-container
     image: blackicebird/2048
     ports:
       - containerPort: 80
       
[cloudshell-user@ip-10-130-1-81 ~]$ kubectl apply -f 2048-pod.yaml
pod/2048-pod created

[cloudshell-user@ip-10-130-1-81 ~]$ kubectl get pods
NAME       READY   STATUS    RESTARTS   AGE
2048-pod   1/1     Running   0          7s

[cloudshell-user@ip-10-130-1-81 ~]$ nano my-game service.yaml

apiVersion: v1
kind: Service
metadata:
   name: mygame-svc
spec:
   selector:
      app: 2048-ws
   ports:
   - protocol: TCP
     port: 80
     targetPort: 80
   type: LoadBalancer
   
[cloudshell-user@ip-10-130-1-81 ~]$ kubectl apply -f service.yaml
service/mygame-svc created

[cloudshell-user@ip-10-130-1-81 ~]$ kubectl describe svc mygame-svc
Name:                     mygame-svc
Namespace:                default
Labels:                   <none>
Annotations:              <none>
Selector:                 app=2048-ws
Type:                     LoadBalancer
IP Family Policy:         SingleStack
IP Families:              IPv4
IP:                       10.100.68.86
IPs:                      10.100.68.86
LoadBalancer Ingress:     aff57652d6b584a66bdc5df8a87d545d-182257295.ap-south-1.elb.amazonaws.com
Port:                     <unset>  80/TCP
TargetPort:               80/TCP
NodePort:                 <unset>  32281/TCP
Endpoints:                172.31.3.23:80
Session Affinity:         None
External Traffic Policy:  Cluster
Events:
  Type    Reason                Age   From                Message
  ----    ------                ----  ----                -------
  Normal  EnsuringLoadBalancer  24s   service-controller  Ensuring load balancer
  Normal  EnsuredLoadBalancer   21s   service-controller  Ensured load balancer
